
# åŸºäºé“¾å®¶ç§Ÿæˆ¿æ•°æ®çš„ä¸Šæµ·å¸‚é—µè¡ŒåŒºä¸å¾æ±‡åŒºæˆ¿ä»·ç©ºé—´å¯è§†åŒ–åˆ†æ

## ğŸ“Œ æ‘˜è¦
é€šè¿‡Pythonçˆ¬è™«é‡‡é›†é“¾å®¶ç½‘2700å·¦å³æ¡ç§Ÿæˆ¿æ•°æ®ï¼Œç»“åˆGISä¸çƒ­åŠ›å›¾æŠ€æœ¯ï¼Œåˆ†æåœ°é“ç«™ç‚¹å¯¹æˆ¿ä»·çš„å½±å“ã€‚å…³é”®æŠ€æœ¯ï¼š
- åçˆ¬ç­–ç•¥ï¼šRobotsåè®®æ£€æŸ¥ã€åŠ¨æ€User-Agentè½®æ¢ã€éšæœºå»¶è¿Ÿ
- æ•°æ®è§£æï¼šç»“æ„åŒ–æå–ä»·æ ¼/é¢ç§¯/ç‰‡åŒºç­‰å­—æ®µ
- ç©ºé—´å»ºæ¨¡ï¼šæ„å»º"åœ°é“ç«™è·ç¦»-æˆ¿ä»·æ¢¯åº¦æ¨¡å‹"

## ğŸ¯ é¡¹ç›®ä»·å€¼
è§£å†³ç§Ÿæˆ¿å†³ç­–æ ¸å¿ƒé—®é¢˜ï¼š
```diff
+ é‡åŒ–åœ°é“ç«™ç‚¹å‘¨è¾¹ç§Ÿé‡‘è¡°å‡è§„å¾‹
+ è¯†åˆ«è·¨åŒºæ€§ä»·æ¯”æœ€ä¼˜å±…ä½åŒºåŸŸ
+ æä¾›æ•°æ®é©±åŠ¨çš„ç§Ÿæˆ¿ç­–ç•¥
```

## ğŸ•·ï¸ æ•°æ®çˆ¬å–
### é“¾å®¶ç§Ÿæˆ¿æ•°æ®
- **ç›®æ ‡URL**ï¼š  
  `https://sh.lianjia.com/zufang/minhang/`  
  `https://sh.lianjia.com/zufang/xuhui/`
- **çˆ¬å–æ¡†æ¶**ï¼š
### æ ¸å¿ƒåŠŸèƒ½
```python
# 1. æ•°æ®çˆ¬å–
def get_page(page):
    """è·å–å•é¡µHTML"""
    url = base_url.format(page)
    if not rp.can_fetch('*', url): return None  # éµå®ˆrobots.txt
    time.sleep(random.uniform(15, 20))          # éšæœºå»¶è¿Ÿé˜²å°ç¦
    headers['User-Agent'] = ua.random           # åŠ¨æ€User-Agent
    response = requests.get(url, headers=headers)
    return response.text if response.ok else None

# 2. æ•°æ®è§£æ
def parse_html(html):
    """æå–æˆ¿æºå…³é”®ä¿¡æ¯"""
    for item in soup.find_all('div', class_='content__list--item'):
        title = item.find('a')['title']           # æˆ¿æºæ ‡é¢˜
        price = item.find('span').text.replace('å…ƒ/æœˆ', '')  # æœˆç§Ÿä»·æ ¼
        district = '-'.join([a.text for a in item.find_all('a')])  # ä¸‰çº§ç‰‡åŒº
        area = re.search(r'\d+\.?\d*ã¡', item.text).group()  # ä½¿ç”¨é¢ç§¯
        # ... æ„å»ºç»“æ„åŒ–æ•°æ®å­—å…¸ ...

# 3. æ•°æ®å­˜å‚¨
def save_data(data):
    """ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    with open('é“¾å®¶ç§Ÿæˆ¿æ•°æ®.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```
### åœ°é“ç«™ç‚¹æ•°æ®
<!-- ![åœ°é“æ•°æ®é‡‡é›†æµç¨‹](media/image6.png) -->
- **æ•°æ®æº**ï¼šé«˜å¾·åœ°å›¾Ajaxè¯·æ±‚é€†å‘åˆ†æï¼ˆ367ä¸ªç«™ç‚¹ï¼‰
- **å…³é”®å­—æ®µ**ï¼š
  ```json
  {
    "line_name": "1å·çº¿",
    "station_name": "è˜åº„",
    "longitude": 121.3852,
    "latitude": 31.1129,
    "is_transfer": false
  }
  ```

## ğŸ§¹ æ•°æ®é¢„å¤„ç†
### åœ°ç†ç¼–ç æµç¨‹
```mermaid
graph TD
    A[åŸå§‹æ•°æ®] --> B{åœ°å€æ ‡å‡†åŒ–}
    B -->|æˆåŠŸ| C[é«˜å¾·Geocoding API]
    B -->|å¤±è´¥| D[äººå·¥æ ¡éªŒ]
    C --> E[ç»çº¬åº¦åæ ‡]
```
### å…³é”®è®¡ç®—
- **Haversineè·ç¦»å…¬å¼**ï¼š
  ```math
  d = 2R \cdot \arcsin\left(\sqrt{\sin^2\left(\frac{\Delta\phi}{2}\right) + \cos\phi_1\cos\phi_2\sin^2\left(\frac{\Delta\lambda}{2}\right)}\right)
  ```
- **è¡ç”Ÿå˜é‡**ï¼š
  - `å•ä»· = æœˆç§Ÿé‡‘ / é¢ç§¯`
  - `æœ€è¿‘åœ°é“ç«™è·ç¦»`
## æœ€ç»ˆæ•°æ®æ ·å¼
![æ•°æ®æ ·å¼](media\datasample.png)
## ğŸ“Š å¯è§†åŒ–åˆ†æ
### æ ¸å¿ƒå›¾è¡¨
| åˆ†æç»´åº¦ | å¯è§†åŒ–å±•ç¤º |
|---------|------------|
| ä»·æ ¼åˆ†å¸ƒ | ![ç›´æ–¹å›¾](media/price.png) | 
| åˆ°æœ€è¿‘åœ°é“ç«™è·ç¦» | ![ç›´æ–¹å›¾](media/Distance.png) | 
| çƒ­åŠ›å›¾ | ![çƒ­åŠ›å›¾](media/heatmap.png) |
| åœ°é“æº¢ä»· | ![æ•£ç‚¹å›¾](media/regression.png) | 
| ä»·æ ¼åŒºé—´ | ![é¥¼å›¾](media/piechart.png) | 
| ç®±å‹å›¾ | ![ç®±å‹å›¾](media/boxchart.png) |




## ğŸ“‚ æ•°æ®æ–‡ä»¶
- `æ•°æ®\CSV\house_data.csv`ï¼ˆé—µè¡Œå’Œå¾æ±‡åŒºç§Ÿæˆ¿æ•°æ®ï¼‰
- `æ•°æ®\JSON\subway_stations.json`ï¼ˆåœ°é“ç«™ç‚¹åœ°ç†æ•°æ®ï¼‰
- `Heatmap\data.js`ï¼ˆçƒ­åŠ›å›¾æ•°æ®ï¼‰

## âš ï¸ æ³¨æ„äº‹é¡¹
1. çƒ­åŠ›å›¾ä½¿ç”¨å¯¹æ•°æƒé‡ï¼š`weight = log(price)-6`
2. åœ°é“è·ç¦»è®¡ç®—å«åœ°çƒæ›²ç‡ä¿®æ­£
3. æ•°æ®é‡‡é›†æ—¶é—´ï¼š2025å¹´6æœˆ

## å°šæœªå®Œæˆ
æ–°æˆ¿æ•°æ®

---
> ğŸ“Š å®Œæ•´åˆ†æä»£ç è§ï¼š[Githubä»“åº“é“¾æ¥]  